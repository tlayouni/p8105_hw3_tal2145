---
title: "p8105_hw3_tal2145"
author: Troy Layouni
date: 2019-10-08
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Loading packages**

```{r}
library(tidyverse)
library(ggridges)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

**Problem 1, Part 1: Loading Instacart data**

```{r}
library(p8105.datasets)
data("instacart")
```


**Problem 1, Part 2: Number of aisles and the aisle with the most items ordered**

```{r}
instacart_aisles = 
  instacart %>% 
  group_by(aisle) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n)) 

instacart_aisles
```

There are 134 aisles in the instacart dataset and the aisle that the most items are ordered from is the fresh vegetables. 

**Problem 1, Part 3: Plotting the number of items ordered in each aisle, limited to aisles with more than 10,000 items ordered**

```{r}
instacart_aisles %>% 
  filter(n > 10000) %>% 
  mutate(aisle = reorder(aisle, n)) %>% 
  
ggplot(aes(x = aisle, y = n)) +
  geom_col() + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(
   title = "Number of Items Ordered by Aisle",
   x = "Aisle",
   y = "Number of Items Ordered",
   caption = "Data from Instacart"
  ) 
```

**Problem 1, Part 4: Creating a table to show the 3 most popular items from each table**

```{r}
  instacart %>% 
  group_by(product_name, aisle) %>% 
  summarize(n = n()) %>%
  #arrange(desc(n)) %>% 
  #top_n(n = 3) %>% 
  group_by(aisle) %>% 
  filter(
    aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits"), min_rank(desc(n)) < 4) %>% 
  arrange(n, aisle) %>% 
  knitr::kable(caption = "3 Most Popular Items by Aisle")
```

**Problem 1, Part 5: Creating a table for the mean hour of the day that both Pink Lady Apples and Coffee Ice Cream are ordered each day of the week ** 

```{r}
instacart %>% 
    mutate(order_dow = recode(order_dow, "0" = "Sunday", "1" = "Monday", "2" = "Tuesday", "3" = "Wednesday", "4" = "Thursday", "5" = "Friday", "6" = "Saturday")) %>% 
  group_by(product_name, order_dow) %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  mutate(mean_hour = mean(order_hour_of_day)) %>% 
  select(product_name, order_dow, mean_hour) %>% 
  distinct() %>% 
  pivot_wider(
    names_from = "order_dow",
    values_from = c("mean_hour")
  ) %>% 
  select(product_name, Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday) %>% 
  knitr::kable(caption = "Instacart: Mean Hour Products are Bought by Day of Week")
```

## Problem 2

**Problem 2, Part 1: Loading data and data cleaning**

* Loading BRFSS from the `p8105.datasets` package
* Cleaning by: 
  * Formating to appropriate variable names (snake_case)
  * Filtering to only the "Overall Health" topic
    * This includes only responses from "Excellent" to "Poor"
  * Organizing responses as a factor taking levels ordered from "Poor" to "Excellent"
  
```{r}
library(p8105.datasets)
data("brfss_smart2010")

brfss_smart2010 =
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = fct_relevel(as.factor(response), c("Poor", "Fair", "Good", "Very good", "Excellent"))) 
```

**Problem 2, part 2: States with 7 or more locations**

```{r}
brfss_smart2010 %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>% 
  distinct(locationdesc) %>% 
  count(locationabbr, name = "n_locations") %>% 
  filter(n_locations > 6)
```

In 2002, Connecticut, Florida, Massachusetts, North Carolina, New Jersey and Pennsylvania had at least 7 locations 


```{r}
brfss_smart2010 %>% 
  filter(year == 2010) %>% 
  group_by(locationabbr) %>% 
  distinct(locationdesc) %>% 
  count(locationabbr, name = "n_locations") %>% 
  filter(n_locations > 6)
```

In 2010 far more states had at least 7 locations which included California, Colorado, Florida, Massachusetts, Maryland, North Carolina, Nebraska, New Jersey, New York, Ohio, Pennsylvania,South Carolina, Texas and Washington. 


Problem 2, part 3: Constructing a new data set 
* Filtered to only include responses of "Excellent" 
* Selecting variables year and state 
* Creating a variable for the averages of the `data_value` across locations within a state 

```{r}
state_value_data = 
  brfss_smart2010 %>% 
  filter(response == "Excellent") %>%
  group_by(locationabbr, year) %>%
  mutate(state_value_mean = mean(data_value, na.rm = TRUE)) %>% 
  select(year, locationabbr, state_value_mean) %>% 
  distinct() 
```

Constructing a spaghetti plot of average value over time within a state

```{r}
state_value_data %>% 
  group_by(locationabbr) %>% 
  
ggplot(aes(x = year, y = state_value_mean, color = locationabbr)) +
  geom_line() +
    labs(
   title = "Mean Data Value by State",
   x = "Year",
   y = "Mean Data Value",
   caption = "Data from BRFSS"
  ) 
```

**Problem 2, part 3: Making a two-panel plot showing the years 2006 and 2010 distribution of data_vlaue for responses "Poor" to "Excellent" among locations in New York**

```{r}
brfss_smart2010 %>% 
  filter(year %in% c("2006", "2010"), locationabbr == "NY") %>%
  group_by(locationdesc, response) %>% 
  
  ggplot(aes(x = response, y = data_value, fill = locationdesc)) +
  geom_col() + 
  facet_grid(~year) +
  viridis::scale_fill_viridis(discrete = TRUE) +
      labs(
   title = "Data Value by Response in all New York Locations: 2006 and 2010",
   x = "Response",
   y = "Data Value",
   caption = "Data from BRFSS"
  ) 
```

## Problem 3

**Problem 3, part 1: Tidying accelerometer data 

```{r}
accelerometer_data = 
read_csv(file = "./data/accel_data.csv") %>%
  janitor::clean_names() %>% 
  pivot_longer(
    cols = starts_with("activity"),
    names_to = "minute",
    values_to = "activity_count",
    names_prefix = "activity_"
  ) %>% 
  mutate(minute = as.integer(minute),
         day_type = case_when(
           day %in% c("Saturday", "Sunday") ~ "weekend",
           day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday") ~ "weekday",
           TRUE ~ ""
   ))
```

The `accelerometer_data` dataset includes `r ncol(pull(accelerometer_data))`

   




