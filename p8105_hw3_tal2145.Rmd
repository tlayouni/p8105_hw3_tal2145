---
title: "p8105_hw3_tal2145"
author: Troy Layouni
date: 2019-10-08
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Loading packages**

```{r load_packages}
library(tidyverse)
library(ggridges)
library(viridis)
library(lubridate)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

**Problem 1, Part 1: Loading Instacart data and exploration**

```{r load_instacart_data}
library(p8105.datasets)
data("instacart")
```

The `instacart` dataset is a dataset with `r ncol(instacart)` columns and `r nrow(instacart)` rows, each indicating a particular item ordered by a customer using instacart. This data includes information from `r nrow(distinct(instacart, user_id))` unique users and includes variables on the day of week an order was made (`order_dow`), hour of the day the order was placed (`order_hour_of_day`), the order sequence of items, and the aisle and department each ordered item is located in. Among customers who have placed prior orders, the average number of days since a customer's last order is `r mean(pull(distinct(instacart, user_id, .keep_all = TRUE), days_since_prior_order))` days but this variable was capped at 30 days. The department with the most items ordered was `r names(which.max(table(pull(instacart, department))))` and avocados were ordered a total of `r nrow(filter(instacart, product_name == "Organic Hass Avocado"))` times. 

**Problem 1, Part 2: Number of aisles and the aisle with the most items ordered**

```{r instacart_aisles}
instacart_aisles = 
  instacart %>% 
  group_by(aisle) %>% 
  summarize(n_items_ordered = n()) %>% 
  arrange(desc(n_items_ordered))

instacart_aisles
```

There are `r nrow(distinct(instacart, aisle))` aisles in the instacart dataset and the aisle that the most items are ordered from is the `r names(which.max(table(pull(instacart, aisle))))` followed by fresh fruits then packaged vegetables fruits.  

**Problem 1, Part 3: Plotting the number of items ordered in each aisle, limited to aisles with more than 10,000 items ordered**

```{r instacart_plot}
instacart_aisles %>% 
  filter(n_items_ordered > 10000) %>% 
  mutate(aisle = reorder(aisle, n_items_ordered)) %>% 
  
ggplot(aes(x = aisle, y = n_items_ordered)) +
  geom_col() + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(
   title = "Number of Items Ordered by Aisle",
   x = "Aisle",
   y = "Number of Items Ordered",
   caption = "Data from Instacart"
  ) 
```

The graph is order by increasing number of items ordered by aisle. We see that the aisle with the fewest items ordered among the aisle with more than 10,000 ordered items is `butter`, but by large the aisles with the highest ordered items, `fresh vegetables`, `fresh fruits`, and `packaged vegetables` have much larger number of orders (around 150,000 for fruits and fresh vegetables). 

**Problem 1, Part 4: Creating a table to show the 3 most popular items from each table**

```{r instacart_table}
  instacart %>% 
  group_by(product_name, aisle) %>% 
  summarize(n_items = n()) %>%
  group_by(aisle) %>% 
  filter(
    aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits"), min_rank(desc(n_items)) < 4) %>% 
  arrange(n_items, aisle) %>% 
  knitr::kable(caption = "3 Most Popular Items by Aisle")
```

Relative to the other two aisles, the `dog food care` aisle has very few items ordered via instacart. The most popular item in the `dog food aisle` is `Snack Sticks Chicken & Rice Recipe Dog Treats` and only 30 were ordered by the customers in the instacart dataset. Unlike the `dog food care` aisle, the most popular item in the `packaged vegetables fruits` aisle has 9784 orders placed for `Organic Baby Spinach`. The most popular item in the `baking ingredients` aisle is `Light Brown Sugar`.

**Problem 1, Part 5: Creating a table for the mean hour of the day that both Pink Lady Apples and Coffee Ice Cream are ordered each day of the week** 

```{r instacart_table_hour}
instacart %>% 
  mutate(order_dow = recode(order_dow, "0" = "Sunday", "1" = "Monday", "2" = "Tuesday", "3" = "Wednesday", "4" = "Thursday", "5" = "Friday", "6" = "Saturday")) %>% 
  group_by(product_name, order_dow) %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  mutate(
    mean_hour = mean(order_hour_of_day),
    mean_hour = paste(floor(mean_hour), round((mean_hour - floor(mean_hour))*60), sep = ":")
   ) %>% 
  select(product_name, order_dow, mean_hour) %>% 
  distinct() %>% 
  pivot_wider(
    names_from = "order_dow",
    values_from = c("mean_hour")
  ) %>% 
  select(product_name, Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday) %>% 
  knitr::kable(caption = "Instacart: Mean Hour Products are Bought by Day of Week")
```

When looking at this table comparing the average time both `Pink Lady Apples` and `Coffee Ice Cream` on bought on each day of the week, it looks like on most days, apples are purchased earlier in the day either late morning or early afternoon, and ice cream is purchased slightly later in the mid afternoon. 

## Problem 2

**Problem 2, Part 1: Loading data and data cleaning**

* Loading BRFSS from the `p8105.datasets` package
* Cleaning by: 
  * Formating to appropriate variable names (snake_case)
  * Filtering to only the "Overall Health" topic
    * This includes only responses from "Excellent" to "Poor"
  * Organizing responses as a factor taking levels ordered from "Poor" to "Excellent"
  
```{r load_brfss}
library(p8105.datasets)
data("brfss_smart2010")

brfss_smart2010 =
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename("state" = "locationabbr", "county" = "locationdesc") %>%
  filter(topic == "Overall Health") %>% 
  mutate(response = fct_relevel(as.factor(response), c("Poor", "Fair", "Good", "Very good", "Excellent")))
```

**Problem 2, part 2: States with 7 or more locations**

```{r brfss_7_locations}
brfss_smart2010 %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  distinct(county) %>% 
  count(state, name = "n_locations") %>% 
  filter(n_locations > 6)
```

In 2002, Connecticut, Florida, Massachusetts, North Carolina, New Jersey and Pennsylvania had at least 7 locations for BRFSS.


```{r brfss_10_locations}
brfss_smart2010 %>% 
  filter(year == 2010) %>% 
  group_by(state) %>% 
  distinct(county) %>% 
  count(state, name = "n_locations") %>% 
  filter(n_locations > 6)
```

In 2010 far more states had at least 7 locations which included California, Colorado, Florida, Massachusetts, Maryland, North Carolina, Nebraska, New Jersey, New York, Ohio, Pennsylvania,South Carolina, Texas and Washington. 


**Problem 2, part 3: Constructing a new data set**

* Filtered to only include responses of "Excellent" 
* Selecting variables year and state 
* Creating a variable for the averages of the `data_value` across locations within a state 

```{r excellent_data}
state_value_data = 
  brfss_smart2010 %>% 
  filter(response == "Excellent") %>%
  group_by(state, year) %>%
  mutate(state_value_mean = mean(data_value, na.rm = TRUE)) %>% 
  select(year, state, state_value_mean) %>% 
  distinct() 
```

Constructing a spaghetti plot of average value over time within a state

```{r brfss_plot_mean}
state_value_data %>% 
  group_by(state) %>% 
  
ggplot(aes(x = year, y = state_value_mean, color = state)) +
  geom_line() +
    labs(
   title = "Mean Data Value by State",
   x = "Year",
   y = "Mean Data Value",
   caption = "Data from BRFSS"
  ) 
```

The spaghetti plot is difficult to read by state but the mean data value by state over time appears to be relatively consistent from 2002 to 2010, around 20-25.

**Problem 2, part 3: Making a two-panel plot showing the years 2006 and 2010 distribution of data_vlaue for responses "Poor" to "Excellent" among locations in New York**

```{r brfss_plot_ny}
brfss_smart2010 %>% 
  filter(year %in% c("2006", "2010"), state == "NY") %>%
  group_by(county, response) %>% 
  
  ggplot(aes(x = response, y = data_value, color = response)) +
  geom_boxplot() + 
  facet_grid(~year) +
  viridis::scale_fill_viridis(discrete = TRUE) +
      labs(
   title = "Data Value by Response in all New York Locations: 2006 and 2010",
   x = "Response",
   y = "Data Value",
   caption = "Data from BRFSS"
  ) 
```

In both 2006 and 2010, the data value is higher in responses of `Good`, `Very good` and while still high slightly lower in `Excellent`. The data value in the `Poor` and `Fair` responses are lower and in 2006, the distribution of data values is less spread out than the higher 3 responses. 

## Problem 3

**Problem 3, part 1: Tidying accelerometer data and summarizing new dataset**

* creating a minute and activity count variable
* converting minute variable to an integer type 
* creating a variable for whether a day is a weekend or weekday

```{r load_accelerometer}
accelerometer_data = 
read_csv(file = "./data/accel_data.csv") %>%
  janitor::clean_names() %>% 
  pivot_longer(
    cols = starts_with("activity"),
    names_to = "minute",
    values_to = "activity_count",
    names_prefix = "activity_"
  ) %>% 
  mutate(minute = as.integer(minute),
         day_type = case_when(
           day %in% c("Saturday", "Sunday") ~ "weekend",
           day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday") ~ "weekday",
           TRUE ~ "",
   ))
```

The `accelerometer_data` dataset has `r ncol(accelerometer_data)` columns that include variables for the week, day_id, and day of the week and minute for each observation as well as the activity count at that day and time and whether the observation was on a weekend or weekday. There are `r nrow(accelerometer_data)` rows of observations for every minute of each day the subject was monitored over the 5 weeks. The average activity level over the entire observation period was `r mean(pull(accelerometer_data, activity_count))`. The average activity count on a weekend was `r mean(pull(filter(accelerometer_data, day_type == "weekday"), activity_count)) - mean(pull(filter(accelerometer_data, day_type == "weekend"), activity_count))` less than a weekday. 

**Problem 3, part 2: Creating a table of daily activity**

* Aggregating data across minutes for each day 
* Creating table for results 

```{r accelerometer_table}
accelerometer_data %>% 
  group_by(day, week) %>% 
  mutate(daily_activity = sum(activity_count)) %>% 
  distinct(week, day, daily_activity) %>% 
  pivot_wider(
    names_from = day,
    values_from = daily_activity
  ) %>% 
  select(week, Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday) %>% 
  knitr::kable(caption = "Daily Acitivity by Observation Day")
```


**Problem 3, part 3: Producing a single-panel plot to show activity per day, color coded by day of week 

```{r accelerometer_plot}
accelerometer_data %>% 
  
   ggplot(aes(x = minute, y = activity_count, color = day)) + 
    geom_point(aes(alpha = .3)) +
  scale_x_continuous(
    breaks = c(0, 60, 120, 180, 240, 300, 360, 420, 480, 540, 600, 660, 720, 780, 840, 900, 960, 1020, 1080, 1140, 1200, 1260, 1320, 1380),
    labels = c("12:00 AM", "1:00 AM", "2:00 AM", "3:00 AM", "4:00 AM", "5:00 AM", "6:00 AM", "7:00 AM", "8:00 AM", "9:00 AM", "10:00 AM", "11:00 AM", "12:00 PM", "1:00 PM", "2:00 PM", "3:00 PM", "4:00 PM", "5:00 PM", "6:00 PM", "7:00 PM", "8:00 PM", "9:00 PM", "10:00 PM", "11:00 PM")
  ) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(
   title = "Activity Level Over 24 Hours",
   x = "Time",
   y = "Activity Count",
   caption = "Data from CUMC"
  ) 
```





